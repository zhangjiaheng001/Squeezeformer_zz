{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.sys.path.append('/workspace/Ribonanza-RNA-Folding/Squeezeformer/')\n",
    "#from squeezeformer.model import Squeezeformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os, gc\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix fastai bug to enable fp16 training with dictionaries\n",
    "\n",
    "import torch\n",
    "from fastai.vision.all import *\n",
    "def flatten(o):\n",
    "    \"Concatenate all collections and items as a generator\"\n",
    "    for item in o:\n",
    "        if isinstance(o, dict): yield o[item]; continue\n",
    "        elif isinstance(item, str): yield item; continue\n",
    "        try: yield from flatten(item)\n",
    "        except TypeError: yield item\n",
    "\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "@delegates(GradScaler)\n",
    "class MixedPrecision(Callback):\n",
    "    \"Mixed precision training using Pytorch's `autocast` and `GradScaler`\"\n",
    "    order = 10\n",
    "    def __init__(self, **kwargs): self.kwargs = kwargs\n",
    "    def before_fit(self): \n",
    "        self.autocast,self.learn.scaler,self.scales = autocast(),GradScaler(**self.kwargs),L()\n",
    "    def before_batch(self): self.autocast.__enter__()\n",
    "    def after_pred(self):\n",
    "        if next(flatten(self.pred)).dtype==torch.float16: self.learn.pred = to_float(self.pred)\n",
    "    def after_loss(self): self.autocast.__exit__(None, None, None)\n",
    "    def before_backward(self): self.learn.loss_grad = self.scaler.scale(self.loss_grad)\n",
    "    def before_step(self):\n",
    "        \"Use `self` as a fake optimizer. `self.skipped` will be set to True `after_step` if gradients overflow. \"\n",
    "        self.skipped=True\n",
    "        self.scaler.step(self)\n",
    "        if self.skipped: raise CancelStepException()\n",
    "        self.scales.append(self.scaler.get_scale())\n",
    "    def after_step(self): self.learn.scaler.update()\n",
    "\n",
    "    @property \n",
    "    def param_groups(self): \n",
    "        \"Pretend to be an optimizer for `GradScaler`\"\n",
    "        return self.opt.param_groups\n",
    "    def step(self, *args, **kwargs): \n",
    "        \"Fake optimizer step to detect whether this batch was skipped from `GradScaler`\"\n",
    "        self.skipped=False\n",
    "    def after_fit(self): self.autocast,self.learn.scaler,self.scales = None,None,None\n",
    "        \n",
    "import fastai\n",
    "fastai.callback.fp16.MixedPrecision = MixedPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'example0'\n",
    "PATH = '/workspace/Ribonanza-RNA-Folding/datamount/'\n",
    "OUT = './'\n",
    "bs = 32\n",
    "num_workers = 1\n",
    "SEED = 2023\n",
    "\n",
    "ds = 4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nfolds=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNA_Dataset(Dataset):\n",
    "    def __init__(self, df, mode='train', seed=2023, fold=0, nfolds=4, \n",
    "                 mask_only=False, **kwargs):\n",
    "        self.seq_map = {'A':0,'C':1,'G':2,'U':3}\n",
    "        self.Lmax = 206\n",
    "        df['L'] = df.sequence.apply(len)\n",
    "        df_2A3 = df.loc[df.experiment_type=='2A3_MaP']\n",
    "        df_DMS = df.loc[df.experiment_type=='DMS_MaP']\n",
    "        \n",
    "        split = list(KFold(n_splits=nfolds, random_state=seed, \n",
    "                shuffle=True).split(df_2A3))[fold][0 if mode=='train' else 1] #分成4个fold\n",
    "        df_2A3 = df_2A3.iloc[split].reset_index(drop=True)\n",
    "        df_DMS = df_DMS.iloc[split].reset_index(drop=True)\n",
    "        \n",
    "        m = (df_2A3['SN_filter'].values > 0) & (df_DMS['SN_filter'].values > 0)\n",
    "        df_2A3 = df_2A3.loc[m].reset_index(drop=True)\n",
    "        df_DMS = df_DMS.loc[m].reset_index(drop=True)\n",
    "        \n",
    "        self.seq = df_2A3['sequence'].values\n",
    "        self.L = df_2A3['L'].values\n",
    "        \n",
    "        self.react_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_err_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_error_0' in c]].values\n",
    "        self.react_err_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                'reactivity_error_0' in c]].values\n",
    "        self.sn_2A3 = df_2A3['signal_to_noise'].values\n",
    "        self.sn_DMS = df_DMS['signal_to_noise'].values\n",
    "        self.mask_only = mask_only\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.seq)  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seq[idx]\n",
    "        if self.mask_only:\n",
    "            mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "            mask[:len(seq)] = True\n",
    "            return {'mask':mask},{'mask':mask}\n",
    "        seq = [self.seq_map[s] for s in seq]\n",
    "        seq = np.array(seq)\n",
    "        mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "        mask[:len(seq)] = True\n",
    "        seq = np.pad(seq,(0,self.Lmax-len(seq)))\n",
    "        \n",
    "        react = torch.from_numpy(np.stack([self.react_2A3[idx],\n",
    "                                           self.react_DMS[idx]],-1))\n",
    "        react_err = torch.from_numpy(np.stack([self.react_err_2A3[idx],\n",
    "                                               self.react_err_DMS[idx]],-1))\n",
    "        sn = torch.FloatTensor([self.sn_2A3[idx],self.sn_DMS[idx]])\n",
    "        \n",
    "        return {'seq':torch.from_numpy(seq), 'mask':mask}, \\\n",
    "               {'react':react, 'react_err':react_err,\n",
    "                'sn':sn, 'mask':mask}\n",
    "    \n",
    "class LenMatchBatchSampler(torch.utils.data.BatchSampler):\n",
    "    def __iter__(self):\n",
    "        buckets = [[]] * 100\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            s = self.sampler.data_source[idx]\n",
    "            if isinstance(s,tuple): L = s[0][\"mask\"].sum()\n",
    "            else: L = s[\"mask\"].sum()\n",
    "            L = max(1,L // 16) \n",
    "            if len(buckets[L]) == 0:  buckets[L] = []\n",
    "            buckets[L].append(idx)\n",
    "            \n",
    "            if len(buckets[L]) == self.batch_size:\n",
    "                batch = list(buckets[L])\n",
    "                yield batch\n",
    "                yielded += 1\n",
    "                buckets[L] = []\n",
    "                \n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yielded += 1\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yielded += 1\n",
    "            yield batch\n",
    "            \n",
    "def dict_to(x, device='cuda'):\n",
    "    return {k:x[k].to(device) for k in x}\n",
    "\n",
    "def to_device(x, device='cuda'):\n",
    "    return tuple(dict_to(e,device) for e in x)\n",
    "\n",
    "class DeviceDataLoader:\n",
    "    def __init__(self, dataloader, device='cuda'):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.dataloader:\n",
    "            yield tuple(dict_to(x, self.device) for x in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RNA_Model(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(4,dim)\n",
    "        self.pos_enc = SinusoidalPosEmb(dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "                dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), depth)\n",
    "        self.proj_out = nn.Linear(dim,2)\n",
    "    \n",
    "    def forward(self, x0):\n",
    "        mask = x0['mask']\n",
    "        Lmax = mask.sum(-1).max()\n",
    "        mask = mask[:,:Lmax]\n",
    "        x = x0['seq'][:,:Lmax]\n",
    "        \n",
    "        pos = torch.arange(Lmax, device=x.device).unsqueeze(0)\n",
    "        pos = self.pos_enc(pos)\n",
    "        x = self.emb(x)\n",
    "        x = x + pos\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=~mask)\n",
    "        x = self.proj_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_RNA_Dataset(Dataset):\n",
    "    def __init__(self, df, mode='train', seed=2023, fold=0, nfolds=4, \n",
    "                 mask_only=False, **kwargs):\n",
    "        self.seq_map = {'A':0,'C':1,'G':2,'U':3}\n",
    "        self.Lmax = 206\n",
    "        df['L'] = df.sequence.apply(len)\n",
    "        df_2A3 = df.loc[df.experiment_type=='2A3_MaP']\n",
    "        df_DMS = df.loc[df.experiment_type=='DMS_MaP']\n",
    "        \n",
    "        split = list(KFold(n_splits=nfolds, random_state=seed, \n",
    "                shuffle=True).split(df_2A3))[fold][0 if mode=='train' else 1] #分成4个fold\n",
    "        df_2A3 = df_2A3.iloc[split].reset_index(drop=True)\n",
    "        df_DMS = df_DMS.iloc[split].reset_index(drop=True)\n",
    "        \n",
    "        m = (df_2A3['SN_filter'].values > 0) & (df_DMS['SN_filter'].values > 0)\n",
    "        df_2A3 = df_2A3.loc[m].reset_index(drop=True)\n",
    "        df_DMS = df_DMS.loc[m].reset_index(drop=True)\n",
    "        \n",
    "        self.seq = df_2A3['sequence'].values\n",
    "        self.L = df_2A3['L'].values\n",
    "        \n",
    "        self.react_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                 'reactivity_0' in c]].values\n",
    "        self.react_err_2A3 = df_2A3[[c for c in df_2A3.columns if \\\n",
    "                                 'reactivity_error_0' in c]].values\n",
    "        self.react_err_DMS = df_DMS[[c for c in df_DMS.columns if \\\n",
    "                                'reactivity_error_0' in c]].values\n",
    "        self.sn_2A3 = df_2A3['signal_to_noise'].values\n",
    "        self.sn_DMS = df_DMS['signal_to_noise'].values\n",
    "        self.mask_only = mask_only\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.seq)  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seq[idx]\n",
    "        if self.mask_only:\n",
    "            mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "            mask[:len(seq)] = True\n",
    "            return {'mask':mask},{'mask':mask}\n",
    "        seq = [self.seq_map[s] for s in seq]\n",
    "        seq = np.array(seq)###,dtype=np.float32\n",
    "        mask = torch.zeros(self.Lmax, dtype=torch.bool)\n",
    "        mask[:len(seq)] = True\n",
    "        input_lengths=torch.tensor(len(seq),dtype=torch.int32)\n",
    "        seq = np.pad(seq,(0,self.Lmax-len(seq)))\n",
    "        \n",
    "        react = torch.from_numpy(np.stack([self.react_2A3[idx],\n",
    "                                           self.react_DMS[idx]],-1))\n",
    "        react_err = torch.from_numpy(np.stack([self.react_err_2A3[idx],\n",
    "                                               self.react_err_DMS[idx]],-1))\n",
    "        sn = torch.FloatTensor([self.sn_2A3[idx],self.sn_DMS[idx]])\n",
    "        \n",
    "        return {'inputs':torch.from_numpy(seq), 'input_lengths':input_lengths}, \\\n",
    "               {'react':react, 'react_err':react_err,\n",
    "                'sn':sn, 'mask':mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RelPositionalE' from 'squeezeformer.modules' (/usr/local/lib/python3.10/dist-packages/squeezeformer/modules.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Ribonanza-RNA-Folding/train.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f3230323331313037227d/workspace/Ribonanza-RNA-Folding/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#os.sys.path.append('/workspace/Ribonanza-RNA-Folding/Squeezeformer_zz/')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f3230323331313037227d/workspace/Ribonanza-RNA-Folding/train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msqueezeformer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m RelPositionalE\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'RelPositionalE' from 'squeezeformer.modules' (/usr/local/lib/python3.10/dist-packages/squeezeformer/modules.py)"
     ]
    }
   ],
   "source": [
    "#os.sys.path.append('/workspace/Ribonanza-RNA-Folding/Squeezeformer_zz/')\n",
    "from squeezeformer.modules import RelPositionalE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "from squeezeformer.attention import MultiHeadedSelfAttentionModule\n",
    "from squeezeformer.convolution import ConvModule, DepthwiseConv2dSubsampling, TimeReductionLayer\n",
    "from squeezeformer.modules import FeedForwardModule, ResidualConnectionModule, recover_resolution\n",
    "\n",
    "\n",
    "class SqueezeformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeezeformer encoder first processes the input with a convolution subsampling layer and then\n",
    "    with a number of squeezeformer blocks.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int, optional): Dimension of input vector \n",
    "        encoder_dim (int, optional): Dimension of squeezeformer encoder\n",
    "        num_layers (int, optional): Number of squeezeformer blocks\n",
    "        reduce_layer_index (int, optional): The layer index to reduce sequence length\n",
    "        recover_layer_index (int, optional): The layer index to recover sequence length\n",
    "        num_attention_heads (int, optional): Number of attention heads\n",
    "        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n",
    "        conv_expansion_factor (int, optional): Expansion factor of squeezeformer convolution module\n",
    "        feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n",
    "        attention_dropout_p (float, optional): Probability of attention module dropout\n",
    "        conv_dropout_p (float, optional): Probability of squeezeformer convolution module dropout\n",
    "        conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n",
    "        half_step_residual (bool): Flag indication whether to use half step residual or not\n",
    "    Inputs: inputs, input_lengths\n",
    "        - **inputs** (batch, time, dim): Tensor containing input vector\n",
    "        - **input_lengths** (batch): list of sequence input lengths\n",
    "    Returns: outputs, output_lengths\n",
    "        - **outputs** (batch, out_channels, time): Tensor produces by squeezeformer encoder.\n",
    "        - **output_lengths** (batch): list of sequence output lengths\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 80,\n",
    "        encoder_dim: int = 512,\n",
    "        num_layers: int = 16,\n",
    "        reduce_layer_index: int = 7,\n",
    "        recover_layer_index: int = 15,\n",
    "        num_attention_heads: int = 8,\n",
    "        feed_forward_expansion_factor: int = 4,\n",
    "        conv_expansion_factor: int = 2,\n",
    "        input_dropout_p: float = 0.1,\n",
    "        feed_forward_dropout_p: float = 0.1,\n",
    "        attention_dropout_p: float = 0.1,\n",
    "        conv_dropout_p: float = 0.1,\n",
    "        conv_kernel_size: int = 31,\n",
    "        half_step_residual: bool = False,\n",
    "    ):\n",
    "        super(SqueezeformerEncoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.reduce_layer_index = reduce_layer_index\n",
    "        self.recover_layer_index = recover_layer_index\n",
    "        self.conv_subsample = DepthwiseConv2dSubsampling(in_channels=1, out_channels=encoder_dim)\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(encoder_dim * (((input_dim - 1) // 2 - 1) // 2), encoder_dim),\n",
    "            ###nn.Linear(encoder_dim,encoder_dim)原本的设计是配合conv_subsample的输出用的,可能可以用FeatureExtractor替代\n",
    "            nn.Dropout(p=input_dropout_p),\n",
    "        )\n",
    "        self.time_reduction_layer = TimeReductionLayer()\n",
    "        self.time_reduction_proj = nn.Linear((encoder_dim - 1) // 2, encoder_dim)\n",
    "        self.time_recover_layer = nn.Linear(encoder_dim, encoder_dim)\n",
    "        self.recover_tensor = None\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for idx in range(num_layers):\n",
    "            if idx < reduce_layer_index:\n",
    "                self.layers.append(\n",
    "                    SqueezeformerBlock(\n",
    "                        encoder_dim=encoder_dim,\n",
    "                        num_attention_heads=num_attention_heads,\n",
    "                        feed_forward_expansion_factor=feed_forward_expansion_factor,\n",
    "                        conv_expansion_factor=conv_expansion_factor,\n",
    "                        feed_forward_dropout_p=feed_forward_dropout_p,\n",
    "                        attention_dropout_p=attention_dropout_p,\n",
    "                        conv_dropout_p=conv_dropout_p,\n",
    "                        conv_kernel_size=conv_kernel_size,\n",
    "                        half_step_residual=half_step_residual,\n",
    "                    )\n",
    "                )\n",
    "            elif reduce_layer_index <= idx < recover_layer_index:\n",
    "                self.layers.append(\n",
    "                    ResidualConnectionModule(\n",
    "                        module=SqueezeformerBlock(\n",
    "                            encoder_dim=encoder_dim,\n",
    "                            num_attention_heads=num_attention_heads,\n",
    "                            feed_forward_expansion_factor=feed_forward_expansion_factor,\n",
    "                            conv_expansion_factor=conv_expansion_factor,\n",
    "                            feed_forward_dropout_p=feed_forward_dropout_p,\n",
    "                            attention_dropout_p=attention_dropout_p,\n",
    "                            conv_dropout_p=conv_dropout_p,\n",
    "                            conv_kernel_size=conv_kernel_size,\n",
    "                            half_step_residual=half_step_residual,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.layers.append(\n",
    "                    SqueezeformerBlock(\n",
    "                        encoder_dim=encoder_dim,\n",
    "                        num_attention_heads=num_attention_heads,\n",
    "                        feed_forward_expansion_factor=feed_forward_expansion_factor,\n",
    "                        conv_expansion_factor=conv_expansion_factor,\n",
    "                        feed_forward_dropout_p=feed_forward_dropout_p,\n",
    "                        attention_dropout_p=attention_dropout_p,\n",
    "                        conv_dropout_p=conv_dropout_p,\n",
    "                        conv_kernel_size=conv_kernel_size,\n",
    "                        half_step_residual=half_step_residual,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"Count parameters of encoder\"\"\"\n",
    "        return sum([p.numel for p in self.parameters()])\n",
    "\n",
    "    def forward(self,inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:  #\n",
    "        \"\"\"\n",
    "        Forward propagate a `inputs` for  encoder training.\n",
    "        Args:\n",
    "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
    "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
    "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
    "        Returns:\n",
    "            (Tensor, Tensor)\n",
    "            * outputs (torch.FloatTensor): A output sequence of encoder. `FloatTensor` of size\n",
    "                ``(batch, seq_length, dimension)``\n",
    "            * output_lengths (torch.LongTensor): The length of output tensor. ``(batch)``\n",
    "        \"\"\"\n",
    "        \n",
    "        #outputs, output_lengths = self.conv_subsample(inputs,input_lengths )\n",
    "        outputs = inputs\n",
    "        output_lengths = input_lengths\n",
    "        #outputs = self.input_proj(outputs)\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if idx == self.reduce_layer_index:\n",
    "                self.recover_tensor = outputs\n",
    "                outputs, output_lengths = self.time_reduction_layer(outputs, output_lengths)\n",
    "                outputs = self.time_reduction_proj(outputs)\n",
    "\n",
    "            if idx == self.recover_layer_index:\n",
    "                outputs = recover_resolution(outputs)\n",
    "                length = outputs.size(1)\n",
    "                outputs = self.time_recover_layer(outputs)\n",
    "                outputs += self.recover_tensor[:, :length, :]\n",
    "                output_lengths *= 2\n",
    "\n",
    "            outputs = layer(outputs)\n",
    "\n",
    "        return outputs, output_lengths\n",
    "\n",
    "\n",
    "class SqueezeformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    SqueezeformerBlock is a simpler block structure similar to the standard Transformer block,\n",
    "    where the MHA and convolution modules are each directly followed by a single feed forward module.\n",
    "\n",
    "    Args:\n",
    "        encoder_dim (int, optional): Dimension of squeezeformer encoder\n",
    "        num_attention_heads (int, optional): Number of attention heads\n",
    "        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n",
    "        conv_expansion_factor (int, optional): Expansion factor of squeezeformer convolution module\n",
    "        feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n",
    "        attention_dropout_p (float, optional): Probability of attention module dropout\n",
    "        conv_dropout_p (float, optional): Probability of squeezeformer convolution module dropout\n",
    "        conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n",
    "        half_step_residual (bool): Flag indication whether to use half step residual or not\n",
    "    Inputs: inputs\n",
    "        - **inputs** (batch, time, dim): Tensor containing input vector\n",
    "    Returns: outputs\n",
    "        - **outputs** (batch, time, dim): Tensor produces by squeezeformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_dim: int = 512,\n",
    "        num_attention_heads: int = 8,\n",
    "        feed_forward_expansion_factor: int = 4,\n",
    "        conv_expansion_factor: int = 2,\n",
    "        feed_forward_dropout_p: float = 0.1,\n",
    "        attention_dropout_p: float = 0.1,\n",
    "        conv_dropout_p: float = 0.1,\n",
    "        conv_kernel_size: int = 31,\n",
    "        half_step_residual: bool = False,\n",
    "    ):\n",
    "        super(SqueezeformerBlock, self).__init__()\n",
    "        if half_step_residual:\n",
    "            self.feed_forward_residual_factor = 0.5\n",
    "        else:\n",
    "            self.feed_forward_residual_factor = 1.0\n",
    "\n",
    "        self.sequential = nn.Sequential(\n",
    "            ResidualConnectionModule(\n",
    "                module=MultiHeadedSelfAttentionModule(\n",
    "                    d_model=encoder_dim,\n",
    "                    num_heads=num_attention_heads,\n",
    "                    dropout_p=attention_dropout_p,\n",
    "                ),\n",
    "            ),\n",
    "            nn.LayerNorm(encoder_dim),\n",
    "            ResidualConnectionModule(\n",
    "                module=FeedForwardModule(\n",
    "                    encoder_dim=encoder_dim,\n",
    "                    expansion_factor=feed_forward_expansion_factor,\n",
    "                    dropout_p=feed_forward_dropout_p,\n",
    "                ),\n",
    "                module_factor=self.feed_forward_residual_factor,\n",
    "            ),\n",
    "            nn.LayerNorm(encoder_dim),\n",
    "            ResidualConnectionModule(\n",
    "                module=ConvModule(\n",
    "                    in_channels=encoder_dim,\n",
    "                    kernel_size=conv_kernel_size,\n",
    "                    expansion_factor=conv_expansion_factor,\n",
    "                    dropout_p=conv_dropout_p,\n",
    "                ),\n",
    "            ),\n",
    "            nn.LayerNorm(encoder_dim),\n",
    "            ResidualConnectionModule(\n",
    "                module=FeedForwardModule(\n",
    "                    encoder_dim=encoder_dim,\n",
    "                    expansion_factor=feed_forward_expansion_factor,\n",
    "                    dropout_p=feed_forward_dropout_p,\n",
    "                ),\n",
    "                module_factor=self.feed_forward_residual_factor,\n",
    "            ),\n",
    "            nn.LayerNorm(encoder_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        return self.sequential(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "#from squeezeformer.encoder import SqueezeformerEncoder\n",
    "\n",
    "\n",
    "class Squeezeformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeezeformer incorporates the Temporal U-Net structure, which reduces the cost of the\n",
    "    multi-head attention modules on long sequences, and a simpler block structure of feed-forward module,\n",
    "    followed up by multi-head attention or convolution modules,\n",
    "    instead of the Macaron structure proposed in Conformer.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classification classes\n",
    "        input_dim (int, optional): Dimension of input vector\n",
    "        encoder_dim (int, optional): Dimension of squeezeformer encoder\n",
    "        num_encoder_layers (int, optional): Number of squeezeformer blocks\n",
    "        reduce_layer_index (int, optional): The layer index to reduce sequence length\n",
    "        recover_layer_index (int, optional): The layer index to recover sequence length\n",
    "        num_attention_heads (int, optional): Number of attention heads\n",
    "        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n",
    "        conv_expansion_factor (int, optional): Expansion factor of squeezeformer convolution module\n",
    "        feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n",
    "        attention_dropout_p (float, optional): Probability of attention module dropout\n",
    "        conv_dropout_p (float, optional): Probability of squeezeformer convolution module dropout\n",
    "        conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n",
    "        half_step_residual (bool): Flag indication whether to use half step residual or not\n",
    "    Inputs: inputs\n",
    "        - **inputs** (batch, time, dim): Tensor containing input vector\n",
    "        - **input_lengths** (batch): list of sequence input lengths\n",
    "    Returns: outputs, output_lengths\n",
    "        - **outputs** (batch, out_channels, time): Tensor produces by squeezeformer.\n",
    "        - **output_lengths** (batch): list of sequence output lengths\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        input_dim: int = 80,\n",
    "        encoder_dim: int = 512,\n",
    "        num_encoder_layers: int = 16,\n",
    "        reduce_layer_index: int = 70,\n",
    "        recover_layer_index: int = 150,\n",
    "        num_attention_heads: int = 8,\n",
    "        feed_forward_expansion_factor: int = 4,\n",
    "        conv_expansion_factor: int = 2,\n",
    "        input_dropout_p: float = 0.1,\n",
    "        feed_forward_dropout_p: float = 0.1,\n",
    "        attention_dropout_p: float = 0.1,\n",
    "        conv_dropout_p: float = 0.1,\n",
    "        conv_kernel_size: int = 31,\n",
    "        half_step_residual: bool = False,\n",
    "    ) -> None:\n",
    "        super(Squeezeformer, self).__init__()\n",
    "        self.encoder = SqueezeformerEncoder(\n",
    "            input_dim=input_dim,\n",
    "            encoder_dim=encoder_dim,\n",
    "            num_layers=num_encoder_layers,\n",
    "            reduce_layer_index=reduce_layer_index,\n",
    "            recover_layer_index=recover_layer_index,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            feed_forward_expansion_factor=feed_forward_expansion_factor,\n",
    "            conv_expansion_factor=conv_expansion_factor,\n",
    "            input_dropout_p=input_dropout_p,\n",
    "            feed_forward_dropout_p=feed_forward_dropout_p,\n",
    "            attention_dropout_p=attention_dropout_p,\n",
    "            conv_dropout_p=conv_dropout_p,\n",
    "            conv_kernel_size=conv_kernel_size,\n",
    "            half_step_residual=half_step_residual,\n",
    "        )\n",
    "        \n",
    "        self.token_embeddings = nn.Embedding(4,encoder_dim)\n",
    "        #self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_d)\n",
    "        \n",
    "        self.fc = nn.Linear(encoder_dim, 2)\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"Count parameters of encoder\"\"\"\n",
    "        return self.encoder.count_parameters()\n",
    "\n",
    "    def forward(self,x) -> Tuple[Tensor, Tensor]:  #inputs: Tensor, input_lengths: Tensor\n",
    "        \"\"\"\n",
    "        Forward propagate a `inputs` and `targets` pair for training.\n",
    "        Args:\n",
    "            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n",
    "                `FloatTensor` of size ``(batch, seq_length, dimension)``.\n",
    "            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)``\n",
    "        Returns:\n",
    "            * predictions (torch.FloatTensor): Result of model predictions.\n",
    "        \"\"\"\n",
    "        inputs = x['inputs'] \n",
    "        inputs =self.token_embeddings(inputs)\n",
    "        input_lengths = x['input_lengths']\n",
    "        encoder_outputs, encoder_output_lengths = self.encoder(inputs,input_lengths)\n",
    "        outputs = self.fc(encoder_outputs)\n",
    "        #outputs = F.log_softmax(outputs, dim=-1)\n",
    "        return outputs #, encoder_output_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred,target):\n",
    "    p = pred[target['mask'][:,:pred.shape[1]]]\n",
    "    y = target['react'][target['mask']].clip(0,1)\n",
    "    loss = F.l1_loss(p, y, reduction='none')\n",
    "    loss = loss[~torch.isnan(loss)].mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "class MAE(Metric):\n",
    "    def __init__(self): \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self): \n",
    "        self.x,self.y = [],[]\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        x = learn.pred[learn.y['mask'][:,:learn.pred.shape[1]]]\n",
    "        y = learn.y['react'][learn.y['mask']].clip(0,1)\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        x,y = torch.cat(self.x,0),torch.cat(self.y,0)\n",
    "        loss = F.l1_loss(x, y, reduction='none')\n",
    "        loss = loss[~torch.isnan(loss)].mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/32 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='60' class='' max='4255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.41% [60/4255 00:12&lt;14:27 0.3774]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Ribonanza-RNA-Folding/train.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f3230323331313037227d/workspace/Ribonanza-RNA-Folding/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m learn \u001b[39m=\u001b[39m Learner(data, model, loss_func\u001b[39m=\u001b[39mloss,cbs\u001b[39m=\u001b[39m[GradientClip(\u001b[39m3.0\u001b[39m)],\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f3230323331313037227d/workspace/Ribonanza-RNA-Folding/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m             metrics\u001b[39m=\u001b[39m[MAE()])\u001b[39m.\u001b[39mto_fp16() \n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f3230323331313037227d/workspace/Ribonanza-RNA-Folding/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#fp16 doesn't help at P100 but gives x1.6-1.8 speedup at modern hardware\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f3230323331313037227d/workspace/Ribonanza-RNA-Folding/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m learn\u001b[39m.\u001b[39;49mfit_one_cycle(\u001b[39m32\u001b[39;49m, lr_max\u001b[39m=\u001b[39;49m\u001b[39m5e-4\u001b[39;49m, wd\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m, pct_start\u001b[39m=\u001b[39;49m\u001b[39m0.02\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f3230323331313037227d/workspace/Ribonanza-RNA-Folding/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m torch\u001b[39m.\u001b[39msave(learn\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstate_dict(),os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(OUT,\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfname\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f3230323331313037227d/workspace/Ribonanza-RNA-Folding/train.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    116\u001b[0m lr_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([h[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mhypers])\n\u001b[1;32m    117\u001b[0m scheds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[39m/\u001b[39mdiv, lr_max, lr_max\u001b[39m/\u001b[39mdiv_final),\n\u001b[1;32m    118\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mmom\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, \u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoms \u001b[39mif\u001b[39;00m moms \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m moms))}\n\u001b[0;32m--> 119\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(n_epoch, cbs\u001b[39m=\u001b[39;49mParamScheduler(scheds)\u001b[39m+\u001b[39;49mL(cbs), reset_opt\u001b[39m=\u001b[39;49mreset_opt, wd\u001b[39m=\u001b[39;49mwd, start_epoch\u001b[39m=\u001b[39;49mstart_epoch)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mset_hypers(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39mif\u001b[39;00m lr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch \u001b[39m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_fit, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelFitException, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_end_cleanup)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch, \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelEpochException)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch_train()\n\u001b[1;32m    248\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch_train\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_batches, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelTrainException)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_batches\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_batch(\u001b[39m*\u001b[39;49mo)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_one_batch, \u001b[39m'\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelBatchException)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mafter_loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myb): \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_grad_opt()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:211\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_grad_opt\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward, \u001b[39m'\u001b[39;49m\u001b[39mbackward\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelBackwardException)\n\u001b[1;32m    212\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_with_events(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step, \u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m, CancelStepException)\n\u001b[1;32m    213\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    200\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._backward\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_backward\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_grad\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed_everything(SEED)\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "df = pd.read_parquet(os.path.join(PATH,'train_data.parquet'))\n",
    "\n",
    "for fold in [0,1,2,3]: #range(ds): # running multiple folds at kaggle may cause OOM\n",
    "    ds_train = my_RNA_Dataset(df, mode='train', fold=fold, nfolds=nfolds)\n",
    "    ds_train_len = my_RNA_Dataset(df, mode='train', fold=fold, \n",
    "                nfolds=nfolds, mask_only=True)\n",
    "    sampler_train = torch.utils.data.RandomSampler(ds_train_len)\n",
    "    len_sampler_train = LenMatchBatchSampler(sampler_train, batch_size=bs,\n",
    "                drop_last=True)\n",
    "    dl_train = DeviceDataLoader(torch.utils.data.DataLoader(ds_train, \n",
    "                batch_sampler=len_sampler_train, num_workers=num_workers,\n",
    "                persistent_workers=True), device)\n",
    "\n",
    "    ds_val = my_RNA_Dataset(df, mode='eval', fold=fold, nfolds=nfolds)\n",
    "    ds_val_len = my_RNA_Dataset(df, mode='eval', fold=fold, nfolds=nfolds, \n",
    "               mask_only=True)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(ds_val_len)\n",
    "    len_sampler_val = LenMatchBatchSampler(sampler_val, batch_size=bs, \n",
    "               drop_last=False)\n",
    "    dl_val= DeviceDataLoader(torch.utils.data.DataLoader(ds_val, \n",
    "               batch_sampler=len_sampler_val, num_workers=num_workers), device)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    data = DataLoaders(dl_train,dl_val)\n",
    "    model = Squeezeformer(num_classes=1)\n",
    "    model = model.to(device)\n",
    "    learn = Learner(data, model, loss_func=loss,cbs=[GradientClip(3.0)],\n",
    "                metrics=[MAE()]).to_fp16() \n",
    "    #fp16 doesn't help at P100 but gives x1.6-1.8 speedup at modern hardware\n",
    "\n",
    "    learn.fit_one_cycle(32, lr_max=5e-4, wd=0.05, pct_start=0.02)\n",
    "    torch.save(learn.model.state_dict(),os.path.join(OUT,f'{fname}_{fold}.pth'))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
